# Sécurité des LLM : Comprendre les risques pour mieux s’en protéger

### Sélection de ressources sur la sécurité des LLMs :

<ins>Blog et tutoriels :</ins>
- [Embrace The Red](https://embracethered.com/blog/), Johann Rehberger
- [learnprompting.org](https://learnprompting.org/docs/introduction)

<ins>Guides et référentiels :</ins>
- [OWASP Top 10 for LLM Applications](https://genai.owasp.org/resource/owasp-top-10-for-llm-applications-2025/), 2025.
- [Phare benchmark](https://phare.giskard.ai/), Giskard, 2025.

<ins>Publications scientifiques (LLM) :</ins>
- Chowdhury et al., [*Breaking Down the Defenses: A Comparative Survey of Attacks on Large Language Models*](https://arxiv.org/pdf/2403.04786), 2024.
- Shen et al,[ *"Do Anything Now" : Characterizing and Evaluating In-The-Wild Jailbreak Prompts on LLM*](https://arxiv.org/pdf/2308.03825), 2023.
- Liu et al, [*Jailbreaking ChatGPT via Prompt Engineering : An Empirical Study*](https://arxiv.org/pdf/2305.13860), 2023.
- Perz et al, [*Ignore Previous Prompt: Attach Techniques For Language Models*](https://openreview.net/pdf?id=qiaRo_7Zmug), 2022.

<ins>Publications scientifiques (Multimodalité) :</ins>
- Liu et al, [*A Survey of Attacks on Large Vision-Language
Models: Resources, Advances, and Future Trends*](https://arxiv.org/pdf/2407.07403), 2024.
- Shayegani et al, [*Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models*](https://arxiv.org/pdf/2307.14539), 2023.
- Bagdasaryan et al, [*Abusing Images and Sounds for
Indirect Instruction Injection in Multi-Modal LLMs*](https://arxiv.org/pdf/2307.10490), 2023.
